{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion MNIST Competition",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tenorchops04/CS217/blob/main/Fashion_MNIST_Competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHwxGemYV_FT"
      },
      "source": [
        "# Fashion MNIST Competition!\n",
        "**Author**: W. Tod Newman\n",
        "\n",
        "**Updates**: Phil Sallee -- Feb 14, 2023\n",
        "\n",
        "## Problem\n",
        "\n",
        "Classify images from the \"Fashion MNIST\" data set.   Optimize the test accuracy.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "This dataset is the Fashion MNIST dataset\n",
        "\n",
        "Researchers at Zalando, an e-commerce company, introduced Fashion MNIST as a drop-in replacement for the original MNIST dataset. Like MNIST, Fashion MNIST consists of a training set consisting of 60,000 examples belonging to 10 different classes and a test set of 10,000 examples. Each training example is a gray-scale image, 28x28 in size. The authors of the work further claim that the Fashion MNIST should actually replace MNIST dataset for benchmarking of new Machine Learning or Computer Vision models.\n",
        "\n",
        "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
        "\n",
        "The Labels are:  \n",
        "0 T-shirt/top\n",
        "1 Trouser\n",
        "2 Pullover\n",
        "3 Dress\n",
        "4 Coat\n",
        "5 Sandal\n",
        "6 Shirt\n",
        "7 Sneaker\n",
        "8 Bag\n",
        "9 Ankle boot \n",
        "\n",
        "## Objective\n",
        "\n",
        "In this competition, you can try different variations of the CNN model given as a reference, you may evaluate techniques to squeeze more performance out of a CNN, or you might even try a completely different model, neural network or otherwise.  You will note that there are tips/tricks/techniques documented in many locations on the internet that could be useful.\n",
        "\n",
        "## Rules and Timeline\n",
        "\n",
        "The primary measure for the competition will be the accuracy of prediction on the test data.  Ties will be broken by Precision accuracy first, then Recall Accuracy if needed.\n",
        "\n",
        "The results will be revealed at the end of the last day of class.  Please submit your Metrics blocks (Starts with SUBMIT... and ends with END SUBMISSION) to instructors (philip.a.sallee@rtx.com) before lunch.\n",
        "\n",
        "Top finisher(s) receive bragging rights, and an opportunity to present their winning approaches to the rest of the class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0A2_Mo0KxWc"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,  Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import TensorBoard,  ModelCheckpoint\n",
        "from keras.layers import LeakyReLU\n",
        "import os\n",
        "%matplotlib inline\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y6VjlzZLKPB"
      },
      "source": [
        "## Set Up Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtL1H333LHGz"
      },
      "source": [
        "epochs = 10                 # Number of Training Epochs\n",
        "num_classes = 10            # This is the number of classes in the Fashion MNIST dataset\n",
        "batch_size = 256            # This parameter can be adjusted\n",
        "img_rows, img_cols = 28, 28 # Pixel sizes of the Images in the Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SW52B7dT9H5"
      },
      "source": [
        "## Gather and Process Fashion MNIST data\n",
        "\n",
        "1. First, collect the data from Keras (our goal is someday that our organizational data is this easy to get!)\n",
        "2. Then split into train and test sets.\n",
        "3. Next we need to process the data into the proper shape for the CNN\n",
        "4. Then scale the floats to land between 0 and 1.  Often times we use sklearn's MinMaxScaler for this, but in this case we're going for simplicity.\n",
        "5. Next take the y_train and y_test labels and encode them one-hot.  This will enable the CNN to function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZENio2YLPUy"
      },
      "source": [
        "# Grab the data from the keras repository\n",
        "\n",
        "mnist_data = fashion_mnist.load_data()\n",
        "x = mnist_data[0][0]\n",
        "y = mnist_data[0][1]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=41)\n",
        "\n",
        "# Process the date into the right tensor shape.  This is a good practice, but\n",
        "# usually tensorflow uses channels last (the 'else' here)\n",
        "\n",
        "if K.image_data_format() == \"channels first\":\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "    \n",
        "#\n",
        "#  Cast to a 32 bit float and then scale so the value is a float between 0 and 1\n",
        "    \n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "#\n",
        "# Convert Class Vector to Binary Class Matrices (one-hot encoding).\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_test.shape)\n",
        "\n",
        "#\n",
        "# Function to decode one-hot encoding later on when we want to evaluate performance.\n",
        "def decode_one_hot(y):\n",
        "    y_classes = [np.argmax(yi, axis=None, out=None) for yi in y]\n",
        "    return y_classes\n",
        "\n",
        "'''\n",
        "\n",
        "Below we're experimenting with the Keras ImageDataGenerator.  From my experience, if these parameters\n",
        "are set too aggressively, the loss/accuracy will either never improve or it will take too long to improve.\n",
        "Below is an example of a complex data augmentation regime.  This is just for reference.  See my more simple\n",
        "one at the bottom.\n",
        "\n",
        "    \n",
        "datagen = ImageDataGenerator(rotation_range=0.5, \n",
        "                                 zoom_range=0.1,\n",
        "                                 featurewise_center=True,\n",
        "                                 #featurewise_std_normalization=True,\n",
        "                                 width_shift_range=0.1, \n",
        "                                 height_shift_range=0.1, \n",
        "                                 shear_range=0.1,\n",
        "                                 horizontal_flip=True, \n",
        "                                 fill_mode=\"nearest\")\n",
        "'''\n",
        "#\n",
        "#  Set up our Image Augmentation Data Generator\n",
        "#\n",
        "datagen = ImageDataGenerator(rotation_range=5)\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FcFnUPDLvnO"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "* In this example, we define the below block as a Sequential Model. \n",
        "* See the excellent [Keras Documentation](https://keras.io/guides/sequential_model/) on Sequential Models for info.\n",
        "* Many of these parameters can be experimented with.  The documentation will help you understand how much to experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EWdo8mALXB9"
      },
      "source": [
        "#\n",
        "# This is what is known as a Tensorflow (Keras) Sequential model\n",
        "# We will talk at some level about each of these layer types in class.\n",
        "#\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Conv2D(64, kernel_size=(3,3)))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100))  \n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "my_callbacks = []\n",
        "\n",
        "# Optional: Save a copy of the model with the best accuracy during training\n",
        "#my_callbacks = [ModelCheckpoint('model_out.hdf5', monitor='accuracy', save_best_only=True, save_freq=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvIfjap-L8TO"
      },
      "source": [
        "## Fit and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5U20dV1L-eP"
      },
      "source": [
        "# Flag to determine whether we use Keras' Image augmentation data generator\n",
        "augmentation = False\n",
        "\n",
        "#\n",
        "# Compile the model so we can fit it. Researching loss functions and optimizers\n",
        "# might be a good thing to do.\n",
        "#\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, \n",
        "              optimizer=keras.optimizers.Adadelta(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if not augmentation:\n",
        "    #\n",
        "    # Fit the model.  Once the model is trained we'll evaluate the performance.\n",
        "    print('not using image augmentation')\n",
        "    hist = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=my_callbacks)\n",
        "else:\n",
        "    # fit the model on batches with real-time data augmentation:\n",
        "    hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "              steps_per_epoch=len(x_train) / batch_size, validation_data=(x_test, y_test),\n",
        "              epochs=epochs, verbose=1, callbacks=my_callbacks, workers = 2)\n",
        "\n",
        "\n",
        "score = model.evaluate(x_test, y_test)\n",
        "\n",
        "#\n",
        "# Predict on the test data and pass to metrics function\n",
        "yhat = np.argmax(model.predict(x_test), axis=-1)\n",
        "y_dec = decode_one_hot(y_test)\n",
        "\n",
        "print(\"\\nSUBMIT THIS BLOCK for the Competition\\n\")\n",
        "print(metrics.classification_report(y_dec, yhat))\n",
        "print(\"Testing Loss:\", score[0])\n",
        "print(\"Testing Accuracy:\", score[1])\n",
        "print(\"END SUBMISSION BLOCK\\n\")\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTXLg2MJMFhy"
      },
      "source": [
        "## Plot the accuracy vs. validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Z97VRkMIRh"
      },
      "source": [
        "epoch_list = list(range(1, len(hist.history['accuracy']) + 1))\n",
        "plt.plot(epoch_list, hist.history['accuracy'], epoch_list, hist.history['val_accuracy'])\n",
        "plt.legend((\"Training Accuracy\", \"Validation Accuracy\"))\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_list, hist.history['loss'], epoch_list, hist.history['val_loss'])\n",
        "plt.legend((\"Training Loss\", \"Validation Loss\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-imOV3gbDji"
      },
      "source": [
        "## Visualization of Performance on the Test Set\n",
        "\n",
        "Here is a visualization of how well our classifier can do inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfsF8TlLwarT"
      },
      "source": [
        "import cv2\n",
        "from imutils import build_montages\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# initialize our list of output images\n",
        "images = []\n",
        "\n",
        "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
        "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
        " \n",
        "# randomly select a few testing fashion items\n",
        "for i in np.random.choice(np.arange(0, len(y_test)), size=(16,)):\n",
        "\t# classify the clothing\n",
        "\tprobs = model.predict(x_test[np.newaxis, i])\n",
        "\tprediction = probs.argmax(axis=1)\n",
        "\tlabel = labelNames[prediction[0]]\n",
        " \n",
        "\t# extract the image from the testData if using \"channels_first\"\n",
        "\t# ordering\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\timage = (x_test[i][0] * 255).astype(\"uint8\")\n",
        " \n",
        "\t# otherwise we are using \"channels_last\" ordering\n",
        "\telse:\n",
        "\t\timage = (x_test[i] * 255).astype(\"uint8\")\n",
        "    # initialize the text label color as green (correct)\n",
        "\tcolor = (0, 255, 0)\n",
        " \n",
        "\t# otherwise, the class label prediction is incorrect\n",
        "\tif prediction[0] != np.argmax(y_test[i]):\n",
        "\t\tcolor = (0, 0, 255)\n",
        " \n",
        "\t# merge the channels into one image and resize the image from\n",
        "\t# 28x28 to 96x96 so we can better see it and then draw the\n",
        "\t# predicted label on the image\n",
        "\timage = cv2.merge([image] * 3)\n",
        "\timage = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
        "\tcv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
        "\t\tcolor, 2)\n",
        " \n",
        "\t# add the image to our list of output images\n",
        "\timages.append(image)\n",
        "# construct the montage for the images\n",
        "montage = build_montages(images, (96, 96), (4, 4))[0]\n",
        " \n",
        "# show the output montage\n",
        "cv2_imshow( montage)\n",
        "#cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}